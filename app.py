### python -m streamlit run app.py
### conda activate /Users/happykuma/miniconda3

import os
import requests
from bs4 import BeautifulSoup
import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from openai import OpenAI
from dotenv import load_dotenv
import streamlit as st
from datetime import datetime
import pytz
import urllib.request
import json
import uuid
import re
import ssl
import urllib.parse
import numpy as np
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client_id = os.getenv("client_id")
client_secret = os.getenv("client_secret")
kst = pytz.timezone("Asia/Seoul")
today_kst = datetime.now(kst).strftime("%a, %d %b %Y")
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_MODEL = "gpt-4o-mini"

embedding_function = OpenAIEmbeddingFunction(
    api_key=os.getenv("OPENAI_API_KEY"),
    model_name=EMBEDDING_MODEL
)

###################################
client = OpenAI()

chroma_client = chromadb.PersistentClient(path="./chroma_db")
corpus_collection = chroma_client.get_or_create_collection(
    name='NEWS',
    embedding_function=embedding_function
)

# RSS XMLì—ì„œ ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
def fetch_news_titles(xml_url):
    response = requests.get(xml_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'lxml-xml')

    articles = []
    for item in soup.find_all('item', limit=5):
        title = item.find('title').text
        link = item.find('link').text
        articles.append((title, link))
    return articles

# í…ìŠ¤íŠ¸ chunk ë¶„í•  í•¨ìˆ˜ 
def smart_chunk_splitter(texts, titles, dates, max_chunk_size=1500):
    chunks = []

    if isinstance(texts, str):
        texts = [texts]
        titles = [titles]
        dates = [dates]

    for text, title, date in zip(texts, titles, dates):
        current_chunk = ""
        sentences = text.split('.')

        for sentence in sentences:
            sentence = sentence.strip()

            if not sentence:
                continue

            if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                current_chunk += sentence + '. '

            else:
                current_chunk = f"title : {title}, date : {date}, content : {current_chunk}"
                chunks.append(current_chunk.strip())
                current_chunk = sentence + '. '

        if current_chunk:
            chunks.append(f"title: {title}, date: {date}, content: {current_chunk.strip()}")

    return chunks

# keyword ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ - 1
def create_chat_prompt1(user_query):
    system_prompt = """ ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì—ì„œ ì›¹ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤.

            ğŸ“Œ **ì§€ì¹¨**
            1. **ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´**ë¥¼ í¬ì°©í•œ í‚¤ì›Œë“œë¥¼ **ìµœëŒ€ 2ê°œê¹Œì§€** ì¶”ì¶œí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•˜ë‚˜ ì´ìƒ ì¶”ì¶œí•˜ì„¸ìš”.   
            2. **ë³µí•©ëª…ì‚¬**ëŠ” ê°€ëŠ¥í•œ í•œ **í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ** ì¶”ì¶œí•˜ì„¸ìš”.
            3. í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ **ë¬¸ì¥ ì•ˆì˜ ë‹¨ì–´**ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. 
            4. **ë¶ˆí•„ìš”í•œ ë‹¨ì–´(ì˜ˆ: "ê´€ë ¨", "ë‰´ìŠ¤", "ìš”ì•½", "ì •ë³´", "ì†Œì‹")ëŠ” ì œì™¸í•˜ì„¸ìš”.**
            5. ì¶œë ¥ í˜•ì‹: **í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„**í•´ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ì ‘ë‘ì–´ ì—†ì´ ìˆœìˆ˜í•œ í‚¤ì›Œë“œë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
               ì˜ˆ) `ì†í¥ë¯¼ ê²½ê¸°`

            #Sentence: ì†í¥ë¯¼ ì„ ìˆ˜ê°€ ìµœê·¼ ê²½ê¸°ì—ì„œ ì–´ë–¤ ê²°ê³¼ë¥¼ ëƒˆëŠ”ì§€ ê¶ê¸ˆí•´.
            #Keyword: ì†í¥ë¯¼ ê²½ê¸°

            #Sentence: ìº¡í‹´ì•„ë©”ë¦¬ì¹´ ë³µì¥ì„ í•œ ì‚¬ëŒì´ êµ­íšŒì— ë‚œì…í–ˆë‹¤ê³  í•˜ëŠ”ë°, ê·¸ ì´ìœ ê°€ ë­ì•¼?
            #Keyword: ìº¡í‹´ì•„ë©”ë¦¬ì¹´ ë‚œì…
         
            #Sentence: ì‚¼ì„±, LG ê´€ë ¨ ë‰´ìŠ¤ ì•Œë ¤ì¤˜
            #Keyword: ì‚¼ì„± LG

            #Sentence: ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ì•Œë ¤ì¤˜
            #Keyword: ì˜¤ëŠ˜
            """
    return [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"""
            #Sentence: {user_query}
            #Keyword:
        """
        }
    ]

# ì¿¼ë¦¬ì—ì„œ keyword ì¶”ì¶œ
def generate_keywords(user_query):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_chat_prompt1(user_query),
        max_tokens=1500
    )
    return response.choices[0].message.content.replace("#Keyword:", "").strip() # type: ignore

# naver apië¡œ ë‰´ìŠ¤ ê²€ìƒ‰
def find_document(keyword, num_doc):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    def fetch_news(keyword, client_id, client_secret, num_doc):
        encText = urllib.parse.quote(keyword)
        url = f"https://openapi.naver.com/v1/search/news?query={encText}&display={num_doc}&sort=sim" # displayë¡œ ì˜¤íƒ€ ìˆ˜ì •

        context = ssl._create_unverified_context()

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)

        response = urllib.request.urlopen(request, context=context)
        if response.getcode() != 200:
            print("Error Code:", response.getcode())
            return []

        response_body = response.read().decode('utf-8')
        news_data = json.loads(response_body)

        return news_data.get('items', [])

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    news_items = fetch_news(keyword, client_id, client_secret, num_doc)

    contents = []
    titles = []
    dates = []
    url_pattern = re.compile(r"https://(m|n)\.([a-z]+\.)?naver\.com")

    # Selenium
    def get_mobile_news_content(driver, link):
        try:
            driver.get(link)
            title = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "NewsEndMain_article_head_title__ztaL4"))
            ).text
            content = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "_article_content"))
            ).text
            return title, content

        except Exception as e:
            print(f"Error loading mobile page: {link}, {e}")
            return "Failed to load content"

    # BeautifulSoup
    def get_desktop_news_content(link):
        try:
            page = requests.get(link, headers={"User-Agent": "Mozilla/5.0"})
            if page.status_code != 200:
                print(f"Failed to fetch page: {link} (Status Code: {page.status_code})")
                return "Failed to load content"

            soup = BeautifulSoup(page.content, "html.parser")
            title = soup.find(class_="media_end_head_headline").text.strip() # type: ignore
            article_body = soup.find("div", class_="newsct_article _article_body")
            content = article_body.get_text(strip=True) if article_body else "No content available"
            return title, content

        except Exception as e:
            print(f"Error processing link: {link}, {e}")
            return "Failed to load content"

    for item in news_items:
        link = item.get('link', 'No Link')
        date = item.get('pubDate', 'No Date')

        if not url_pattern.match(link):
            continue

        if link.startswith("https://m."):
            title, content= get_mobile_news_content(driver, link)

        else:
            title, content= get_desktop_news_content(link)

        # ì œëª©ì´ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì— ìˆìœ¼ë©´ ì¤‘ë³µì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        if title in titles:
            continue
        
        pattern = re.compile(r'(<.*?>|\(.*?ê¸°ì.*?\)|\[.*?ê¸°ì.*?\]|ë¬´ë‹¨ ì „ì¬.*?ê¸ˆì§€|â“’.*?\s|â–¶.*?\s|ì˜ìƒ|\[ì¶œì²˜.*?\]|í¬í† |ì•µì»¤|â–².*?\s)')
        cleaned_content = pattern.sub(' ', content)
        cleaned_content = re.sub(r'\s+', ' ', cleaned_content).strip()
        
        contents.append(cleaned_content)
        titles.append(title)
        dates.append(date)

    driver.quit()

    return contents, titles, dates

# QA í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ -2
def create_chat_prompt2(system_prompt, user_query, context_documents):
    if isinstance(context_documents[0], list):
        context_documents = [item for sublist in context_documents for item in sublist]

    return [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': f"""
            Context:
            {" ".join(context_documents)}

            ğŸ“Œ **ì§€ì¹¨**
            1. Contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            2. í•µì‹¬ ìš”ì ì„ ë¨¼ì € ì œì‹œí•œ í›„, ìƒì„¸í•œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.

            ğŸ“Œ **ì¶”ê°€ ë‰´ìŠ¤ ìš”ì²­ ì²˜ë¦¬**
            - "ë˜ ì—†ì–´?", "ë” ë³´ì—¬ì¤˜" ë“± ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ë©´ ì´ì „ ì§ˆë¬¸ì„ ì°¸ê³ í•´ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

            ğŸ“Œ **Contextì— ê´€ë ¨ ë‚´ìš©ì´ ì „í˜€ ì—†ì„ ì‹œì—ë§Œ, ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:**
            - "ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤." ë¬¸êµ¬ ì¶œë ¥
            - ë‹¹ì‹ ì´ ì•„ëŠ” ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            
            ğŸ“Œ **ì¶œì²˜ í‘œê¸°**
            ì¶œì²˜ê°€ ë˜ëŠ” ê¸°ì‚¬ì˜ ì œëª©ê³¼, ë³¸ë¬¸ ë¬¸ì¥ì„ ë‹µë³€ ë§ˆì§€ë§‰ì— "ì¶œì²˜:" ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”. ëª…í™•í•œ ë¬¸ì¥ì„ ë‚¨ê¸°ë©´ ì¢‹ìŠµë‹ˆë‹¤. 
            - ì—¬ëŸ¬ ì¶œì²˜ê°€ ìˆëŠ” ê²½ìš° ê°ê° ëª…ì‹œí•˜ì„¸ìš”.

            Question:
            {user_query}

            Answer(ì¶œì²˜ í¬í•¨):

        """}
    ]

# ë‰´ìŠ¤ ìš”ì•½ í”„ë¡¬í”„íŠ¸
def create_headline_summary_prompt(headline_texts):
    return [
        {'role': 'system', 'content': "ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë‰´ìŠ¤ ìš”ì•½ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤."},
        {'role': 'user', 'content': f"""
            ì•„ë˜ì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ì •ë³´ë§Œ ë‚¨ê¸°ë„ë¡ ìš”ì•½í•˜ì„¸ìš”.
            ê° ë‰´ìŠ¤ëŠ” í•µì‹¬ ì‚¬ê±´, ì£¼ìš” ë°°ê²½, ì¸ê³¼ ê´€ê³„ë¥¼ í¬í•¨í•´ì•¼ í•˜ë©°, ìµœëŒ€ 3ì¤„ë¡œ ìš”ì•½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

            ìš”ì•½ í˜•ì‹:
            1. **ë‰´ìŠ¤ ì œëª©**  
               ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½
            2. **ë‰´ìŠ¤ ì œëª©**  
               ë‰´ìŠ¤ ë‚´ìš© ìš”ì•½
            ...

            ë‰´ìŠ¤ë“¤:
            {headline_texts}

            ìš”ì•½:
        """}
    ]

# í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
def generate_headline_summary(headline_texts):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_headline_summary_prompt(headline_texts), 
        max_tokens=1500
    )
    return response.choices[0].message.content

    
# OpenAI APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(system_prompt, user_query, context_documents):
    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=create_chat_prompt2(system_prompt, user_query, context_documents),
        max_tokens=1500
    )
    return response.choices[0].message.content


# RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def chat_with_rag(system_prompt, user_query):
    query_result = corpus_collection.query(query_texts=[user_query], n_results=6)

    distances = query_result['distances'][0] # type: ignore
    threshold = 1.3

    relevant_docs = [d for d in distances if d <= threshold]

    if len(relevant_docs) < 2:
        st.write("ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        keywords = generate_keywords(user_query)
        
        with st.spinner("ì›¹ì—ì„œ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
            news_list, news_title, news_date = find_document(keywords, num_doc=20)

            chunks = smart_chunk_splitter(news_list, news_title, news_date, max_chunk_size=1500) 
            
            existing_docs = set(corpus_collection.get()['documents'])
            new_chunks = [chunk for chunk in chunks if chunk not in existing_docs]

            def generate_unique_id():
                    return str(uuid.uuid4())

            if new_chunks:
                try:
                    new_ids = [generate_unique_id() for _ in new_chunks]
                    corpus_collection.add(ids=new_ids, documents=new_chunks)
                    
                except Exception as e:
                    st.error(f"âŒ ë‰´ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                st.info("ğŸ” ì¶”ê°€í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

            query_result = corpus_collection.query(query_texts=[keywords], n_results=6)
        ###################################
    response = generate_response(system_prompt, user_query, query_result['documents'])
    

    return response


def fetch_news_from_rss(rss_url, category_name="", summary = False):
    """RSSì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml-xml')

        items = soup.find_all('item')
        category_prefix = f"[ì˜¤ëŠ˜ì˜ {category_name} ë‰´ìŠ¤] " if category_name else ""
        
        texts = []
        if summary:
            texts = [
                f"ì˜¤ëŠ˜ì˜ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ {i+1}: {item.title.text}\n\n{item.find('content:encoded').text if item.find('content:encoded') else item.description.text}"
                for i, item in enumerate(items)
            ]

        chunks = [
            chunk
            for item in items
            for chunk in smart_chunk_splitter(
                item.find('content:encoded').text if item.find('content:encoded') else item.description.text,
                f"{category_prefix}, {item.title.text}", item.pubDate.text,
                max_chunk_size=1500
            )
        ]
        return (texts, chunks) if summary else chunks
    
    except requests.exceptions.RequestException as e:
        st.error(f"âŒ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


###################################

# Streamlit UI
st.set_page_config(page_title="NewSeans | AI ë‰´ìŠ¤ ì±—ë´‡", page_icon="ğŸ“°")
st.markdown('<p style="font-size:20px; margin-bottom: 0;">NewSeansì™€ í•¨ê»˜í•˜ëŠ” ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤</p>', unsafe_allow_html=True)
st.subheader("ë‰´ìŠ¤ ìš”ì•½ë¶€í„° ë¶„ì„ê¹Œì§€! ê¶ê¸ˆí•œ ê±´ ëª¨ë‘ ë‚˜ì—ê²Œ ë¬¼ì–´ë´ ğŸ¤–", divider=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

st.sidebar.subheader("ğŸ”¥ ë‰´ìŠ¤ í—¤ë“œë¼ì¸")
rss_url = "https://www.yonhapnewstv.co.kr/category/news/headline/feed/"
news_articles = fetch_news_titles(rss_url)

for title, link in news_articles:
    st.sidebar.markdown(f"[{title}]({link})")

news_categories = {
    "ìµœì‹ ": "http://www.yonhapnewstv.co.kr/browse/feed/",
    "ì •ì¹˜": "http://www.yonhapnewstv.co.kr/category/news/politics/feed/",
    "ê²½ì œ": "http://www.yonhapnewstv.co.kr/category/news/economy/feed/",
    "ì‚¬íšŒ": "http://www.yonhapnewstv.co.kr/category/news/society/feed/",
    "ì§€ì—­": "http://www.yonhapnewstv.co.kr/category/news/local/feed/",
    "ì„¸ê³„": "http://www.yonhapnewstv.co.kr/category/news/international/feed/",
    "ë¬¸í™”ã†ì—°ì˜ˆ": "http://www.yonhapnewstv.co.kr/category/news/culture/feed/",
    "ìŠ¤í¬ì¸ ": "http://www.yonhapnewstv.co.kr/category/news/sports/feed/"
}

st.sidebar.subheader("ğŸ—‚ï¸ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬")

selected_category = st.sidebar.radio("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", list(news_categories.keys()), index=None)

if selected_category:
    selected_url = news_categories[selected_category]
    category_articles = fetch_news_titles(selected_url)

    with st.sidebar.expander(f"ğŸ“Œ ì´ì‹œê° {selected_category} ë‰´ìŠ¤", expanded=True):
        for title, link in category_articles[:3]:
            st.markdown(f"- [{title}]({link})")

if "page_loaded" not in st.session_state:
    st.session_state.page_loaded = False
    st.session_state.corpus_collection = corpus_collection

if not st.session_state.page_loaded:
    existing_ids = st.session_state.corpus_collection.get()["ids"]
    if existing_ids:
        st.session_state.corpus_collection.delete(ids=existing_ids)

    headline_texts, chunks = fetch_news_from_rss(rss_url, "í—¤ë“œë¼ì¸", summary=True)

    for category, category_rss in news_categories.items():
        category_chunks = fetch_news_from_rss(category_rss, category)
        chunks.extend(category_chunks)

    seen = set()
    new_chunks = [x for x in chunks if not (x in seen or seen.add(x))]

    ids = [str(uuid.uuid4()) for _ in new_chunks]
    corpus_collection.add(ids=ids, documents=new_chunks)
    existing_docs = corpus_collection.get()["documents"]
    

    st.session_state.page_loaded = True

#####################

if "headline_summary" not in st.session_state:
    with st.spinner('AIê°€ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ë¥¼ ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤...'):
        st.session_state.headline_summary = generate_headline_summary(headline_texts)

if "headline_title" not in st.session_state:
    st.session_state.headline_title = "ğŸ“° í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ìš”ì•½"

if "headline_subtitle" not in st.session_state:
    st.session_state.headline_subtitle = "ì˜¤ëŠ˜ì˜ ì£¼ìš” í—¤ë“œë¼ì¸ ë‰´ìŠ¤ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:"

st.header(st.session_state.headline_title)

st.write(st.session_state.headline_subtitle)

st.write(st.session_state.headline_summary)
#####################

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

user_query = st.chat_input("ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”! ğŸ—£ï¸")

if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})

    with st.chat_message("user"):
        st.write(user_query)

    with st.spinner("â³ AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_prompt = """
        ìµœì‹  ë‰´ìŠ¤ ì§ˆë¬¸ì— ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì€ ì •í™•í•œ ë‚ ì§œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.

        ì˜¤ëŠ˜ ë‚ ì§œëŠ” {today_kst} ì…ë‹ˆë‹¤.
        ì•„ë˜ ì œê³µëœ ë¬¸ë§¥(Context)ì—ëŠ” ë¬¸ì„œì˜ ë‚ ì§œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        ë‹µë³€ì„ í•  ë•Œ ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. **ì˜¤ëŠ˜(today)**, **ë‚´ì¼(tomorrow)**, **ì–´ì œ(yesterday)** ê°™ì€ ìƒëŒ€ì  ë‚ ì§œ í‘œí˜„ì„ ì •í™•íˆ í•´ì„í•˜ì„¸ìš”.
        2. ë¬¸ë§¥ì— í¬í•¨ëœ ë‚ ì§œ(date)ë¥¼ í˜„ì¬ ë‚ ì§œ({today_kst})ì™€ ë¹„êµí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ë‚ ì§œ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.
        """

        response_text = chat_with_rag(system_prompt, user_query)

        st.session_state.messages.append({"role": "assistant", "content": response_text})

    with st.chat_message("assistant"):
        st.write(response_text)